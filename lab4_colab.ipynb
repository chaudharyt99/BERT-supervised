{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPmndasU-yB6",
        "outputId": "d0483ee4-1cd3-4bf4-8eac-ba168c1323a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# all the necessary imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T2g9Vjlz-yB7"
      },
      "outputs": [],
      "source": [
        "# set the seed\n",
        "manual_seed = 572\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using bert-base-uncased\n",
        "\n",
        "import transformers\n",
        "\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 10\n",
        "BERT_PATH = 'bert-base-uncased'\n",
        "MODEL_PATH = 'model.bin'\n",
        "TRAINING_FILE = '/content/drive/MyDrive/data/yelp_review/train.tsv'\n",
        "VAL_FILE = '/content/drive/MyDrive/data/yelp_review/val.tsv'\n",
        "TEST_FILE = '/content/drive/MyDrive/data/yelp_review/test.tsv'\n",
        "\n",
        "\n",
        "# initializing bert tokenizer\n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "    BERT_PATH,\n",
        "    do_lower_case=True\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mYMAXa4CmWu",
        "outputId": "f9ddf3f2-a2b3-4a50-e495-b7ef6509dc84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining BERT Model\n",
        "\n",
        "class BERTBaseUncased(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTBaseUncased, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(768, 5)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, o2 = self.bert(\n",
        "            ids,\n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=False\n",
        "        )\n",
        "        bo = self.bert_drop(o2)\n",
        "        output = self.out(bo)\n",
        "        return output"
      ],
      "metadata": {
        "id": "0La1ljPUDoXU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining dataloader\n",
        "\n",
        "class BERTDataset:\n",
        "    def __init__(self, content, rating):\n",
        "        self.content = content\n",
        "        self.rating = rating\n",
        "        self.tokenizer = TOKENIZER\n",
        "        self.max_len = MAX_LEN\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.content)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        content = str(self.content[item])\n",
        "        content = ' '.join(content.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            content,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(\n",
        "                token_type_ids, dtype=torch.long\n",
        "                ),\n",
        "            'ratings': torch.tensor(self.rating[item], dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "hEaNMSsfDsiJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining loss fn and training fn\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    return nn.CrossEntropyLoss()(outputs, targets)\n",
        "\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "\n",
        "    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "        ids = d['ids']\n",
        "        token_type_ids = d['token_type_ids']\n",
        "        mask = d['mask']\n",
        "        ratings = d['ratings']\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        ratings = ratings.to(device, dtype=torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        loss = loss_fn(outputs, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    fin_ratings = []\n",
        "    fin_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            ids = d['ids']\n",
        "            token_type_ids = d['token_type_ids']\n",
        "            mask = d['mask']\n",
        "            ratings = d['ratings']\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            ratings = ratings.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "\n",
        "            outputs = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
        "            fin_ratings.extend(ratings.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
        "\n",
        "    return fin_outputs, fin_ratings\n"
      ],
      "metadata": {
        "id": "Lnz6lI3bDsW9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def run():\n",
        "    best_model = None\n",
        "    df_train = pd.read_csv(TRAINING_FILE, delimiter='\\t').fillna('none')\n",
        "    df_valid = pd.read_csv(VAL_FILE, delimiter='\\t').fillna('none')\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    df_train.rating = le.fit_transform(df_train.rating)\n",
        "    df_valid.rating = le.transform(df_valid.rating)\n",
        "\n",
        "    df_train = df_train.reset_index(drop=True)\n",
        "    df_valid = df_valid.reset_index(drop=True)\n",
        "\n",
        "    train_dataset = BERTDataset(\n",
        "        content=df_train.content.values,\n",
        "        rating=df_train.rating.values\n",
        "    )\n",
        "\n",
        "    valid_dataset = BERTDataset(\n",
        "        content=df_valid.content.values,\n",
        "        rating=df_valid.rating.values\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=TRAIN_BATCH_SIZE,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=VALID_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = BERTBaseUncased()\n",
        "    model.to(device)\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'layerNorm.weight']\n",
        "    optimizer_parameters = [\n",
        "        {'params': [\n",
        "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "         'weight_decay': 0.001\n",
        "        },\n",
        "        {'params': [\n",
        "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "         'weight_decay': 0.0\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    num_train_steps = int(\n",
        "        len(df_train) / TRAIN_BATCH_SIZE * EPOCHS\n",
        "    )\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    best_f1score = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
        "        outputs, targets = eval_fn(valid_data_loader, model, device)\n",
        "\n",
        "        accuracy = metrics.accuracy_score(targets, outputs)\n",
        "        f1score = metrics.f1_score(targets, outputs, average='macro')\n",
        "\n",
        "        print(f'Accuracy Score = {accuracy}')\n",
        "        print(f'F1 (macro avg) Score = {f1score}')\n",
        "\n",
        "        if f1score > best_f1score:\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            best_f1score = f1score\n",
        "            best_model = deepcopy(model)\n",
        "\n",
        "    return best_model"
      ],
      "metadata": {
        "id": "KddW6PR7DsKX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "id": "Ccn62UUUDqLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "354cbfea-1f30-4f99-cbec-6ffd71978a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 7000/7000 [46:54<00:00,  2.49it/s]\n",
            "100%|██████████| 1750/1750 [02:08<00:00, 13.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score = 0.6405714285714286\n",
            "F1 (macro avg) Score = 0.6260333945818849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  6%|▌         | 427/7000 [02:52<44:44,  2.45it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cnmtKtzrDqg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLXz9sYE-yB8"
      },
      "source": [
        "You can adap these two functions for your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKiOVHl_-yB9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train(loader):\n",
        "    total_loss = 0.0\n",
        "    # iterate throught the data loader\n",
        "    num_sample = 0\n",
        "    for batch in loader:\n",
        "        # load the current batch\n",
        "        batch_input = batch.review\n",
        "        batch_output = batch.label\n",
        "\n",
        "        batch_input = batch_input.to(device)\n",
        "        batch_output = batch_output.to(device)\n",
        "        # forward propagation\n",
        "        # pass the data through the model\n",
        "        model_outputs = model(batch_input)\n",
        "        # compute the loss\n",
        "        cur_loss = criterion(model_outputs, batch_output)\n",
        "        total_loss += cur_loss.cpu().item()\n",
        "\n",
        "        # backward propagation (compute the gradients and update the model)\n",
        "        # clear the buffer\n",
        "        optimizer.zero_grad()\n",
        "        # compute the gradients\n",
        "        cur_loss.backward()\n",
        "        # update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        num_sample += batch_output.shape[0]\n",
        "\n",
        "    return total_loss/num_sample\n",
        "\n",
        "# evaluation logic based on classification accuracy\n",
        "def evaluate(loader):\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
        "        for batch in loader:\n",
        "             # load the current batch\n",
        "            batch_input = batch.review\n",
        "            batch_output = batch.label\n",
        "\n",
        "            batch_input = batch_input.to(device)\n",
        "            # forward propagation\n",
        "            # pass the data through the model\n",
        "            model_outputs = model(batch_input)\n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(batch_output.cpu())\n",
        "\n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro')\n",
        "    return accuracy,f1score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAGVU01s-yB9"
      },
      "outputs": [],
      "source": [
        "# funtion for save prediction\n",
        "def out_prediction(first_name, last_name, prediction_list):\n",
        "    \"\"\"\n",
        "    out_prediction takes three input varibles: first_name, last_name, prediction_list\n",
        "    <first_name>, string, your first name, e.g., Tom\n",
        "    <last_name>, string, your last name, e.g., Smith\n",
        "    <prediction_list>, list of string which includes all your predications of TEST samples\n",
        "                        e.g., ['1star','5star','3star']\n",
        "\n",
        "    Generate a file is named with <yourfirstname>_<yourlastname>_PRED.txt in current directory\n",
        "    \"\"\"\n",
        "    output_file = open(\"{}_{}_PRED.txt\".format(first_name,last_name),'w')\n",
        "    for item in prediction_list:\n",
        "        output_file.write(item+\"\\n\")\n",
        "    output_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irbRUXv8-yB9"
      },
      "source": [
        "# Please write code to develop you system. More details are in `Lab4.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3kNgXjm-yB-",
        "outputId": "9aa35b94-ce77-4bbe-da93-13f3f6d16b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fL6s4UdzCAOk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}